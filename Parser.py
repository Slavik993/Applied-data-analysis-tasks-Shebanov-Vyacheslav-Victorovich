
# –°—Å—ã–ª–∫–∞ –Ω–∞ google colab: https://colab.research.google.com/drive/101-Ij_unDwHaY0GNU0IgrIelcbcBDdnx?usp=sharing

import warnings
warnings.filterwarnings("ignore")

import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import time
import random
import re
import string
import pandas as pd
import os

# NLTK - –ü–û–õ–ù–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK...")
# –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –ù–ï–û–ë–•–û–î–ò–ú–´–• –†–ï–°–£–†–°–û–í
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)  # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°
nltk.download('stopwords', quiet=True)
print("NLTK –≥–æ—Ç–æ–≤")

# PYMORPHY2
import pymorphy2
import inspect

# –§–ò–ö–° PYMORPHY2
original_getargspec = inspect.getargspec
def getargspec_patch(func):
    if func.__name__ == '__init__':
        full_args = inspect.getfullargspec(func)
        return full_args.args, full_args.varargs, full_args.varkw, full_args.defaults
    else:
        return original_getargspec(func)
inspect.getargspec = getargspec_patch

morph = pymorphy2.MorphAnalyzer()

# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
GAMES = ["Hollow Knight"]

# –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï URL (–¢–ï–°–¢–ò–†–û–í–ê–ù–ù–´–ï)
GAME_SITES = [
    # ‚úÖ –†–ê–ë–û–¢–ê–Æ–©–ò–ï –°–¢–†–ê–ù–ò–¶–´ –° –ö–û–ù–¢–ï–ù–¢–û–ú (–Ω–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ)
    "https://ru.wikipedia.org/wiki/{}".format(GAMES[0].replace(" ", "_")),  # –í–∏–∫–∏-—Å—Ç—Ä–∞–Ω–∏—Ü–∞
    "https://en.wikipedia.org/wiki/{}".format(GAMES[0].replace(" ", "_")),  # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –≤–∏–∫–∏
    "https://hollowknight.fandom.com/wiki/{}".format(GAMES[0].replace(" ", "_")),  # Fandom
    "https://store.steampowered.com/app/367520/{}".format(GAMES[0].replace(" ", "_").lower()),  # Steam
    "https://www.metacritic.com/game/{}".format(GAMES[0].lower().replace(" ", "-")),  # Metacritic
]

print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞: {len(GAME_SITES)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö")

def create_http_session():
    """HTTP —Å–µ—Å—Å–∏—è"""
    session = requests.Session()
    ua = UserAgent()
    headers = {
        'User-Agent': ua.chrome,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en;q=0.6',
    }
    session.headers.update(headers)
    return session

def fix_url_properly(base_url, href):
    """–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç URL"""
    if not href or len(href) < 5:
        return None
    
    # –û—á–∏—Å—Ç–∫–∞
    href = href.strip()
    href = href.split('#')[0].split('?')[0]
    
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ —Å–ª–µ—à–∏
    while '//' in href:
        href = href.replace('//', '/')
    
    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞
    if href.startswith('/'):
        # –ë–∞–∑–æ–≤—ã–π URL –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        base_clean = base_url.split('?')[0].rstrip('/')
        if not base_clean.endswith('/'):
            base_clean += '/'
        full_url = base_clean + href.lstrip('/')
        return full_url
    
    # –ü–æ–ª–Ω—ã–π URL
    if href.startswith('http'):
        # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ —Å–ª–µ—à–∏
        while '//' in href:
            href = href.replace('//', '/')
        return href
    
    # –°—Ö–µ–º–∞ –±–µ–∑ –¥–æ–º–µ–Ω–∞
    return 'https://' + href

def extract_article_text(soup, game_name):
    """–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ì–û –ö–û–ù–¢–ï–ù–¢–ê"""
    game_words = set(game_name.lower().split())
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞
    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'nav']):
        tag.decompose()
    
    # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title = ""
    for tag in ['h1', 'h2', 'h3']:
        elem = soup.find(tag)
        if elem:
            title = elem.get_text(strip=True)
            title = re.sub(r'\s+', ' ', title).strip()
            if len(title) > 5:
                break
    
    # –ú–ù–û–ì–û–≠–¢–ê–ü–ù–´–ô –ü–û–ò–°–ö –ö–û–ù–¢–ï–ù–¢–ê
    content_selectors = [
        '.article-body', '.post-content', '.entry-content',
        '.article-text', '.content-body', '.main-article',
        '.story-body', '[class*="article"]', '[class*="post"]',
        'article p', 'main p', '.content p'
    ]
    
    full_text_parts = []
    
    for selector in content_selectors:
        elements = soup.select(selector)
        if elements:
            for elem in elements:
                text = elem.get_text(strip=True)
                if 30 < len(text) < 1200:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    text_lower = text.lower()
                    if any(word in text_lower for word in game_words):
                        full_text_parts.append(text)
            if len(full_text_parts) > 10:
                break
    
    # Fallback - –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    if len(full_text_parts) < 3:
        all_paragraphs = soup.find_all('p')
        for p in all_paragraphs:
            text = p.get_text(strip=True)
            if 40 < len(text) < 800 and any(word in text.lower() for word in game_words):
                full_text_parts.append(text)
                if len(full_text_parts) >= 15:
                    break
    
    # –°–±–æ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞
    full_text = " ".join(full_text_parts[:20])  # –ú–∞–∫—Å–∏–º—É–º 20 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    full_text = re.sub(r'\s+', ' ', full_text.strip())
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    if len(full_text) > 200 and any(word in full_text.lower() for word in game_words):
        return {
            'title': title[:150] if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
            'text': full_text[:4000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
            'text_length': len(full_text)
        }
    
    return None

def search_and_extract_http(session, game_name, target_count=5):
    """–ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ø–æ–∏—Å–∫ + –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ"""
    print(f"\nüîç –°–ë–û–† –°–¢–ê–¢–ï–ô –î–õ–Ø '{game_name}' ({target_count} —Ü–µ–ª—å)...")
    
    # –¢–ï–°–¢–û–í–´–ï URL (–ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–û –†–ê–ë–û–¢–ê–Æ–¢)
    test_urls = [
        "https://ru.wikipedia.org/wiki/Hollow_Knight",
        "https://en.wikipedia.org/wiki/Hollow_Knight",
        "https://hollowknight.fandom.com/wiki/Hollow_Knight_Wiki",
        "https://store.steampowered.com/app/367520/Hollow_Knight/",
        "https://www.metacritic.com/game/pc/hollow-knight",
    ]
    
    collected_data = []
    successful = 0
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n  #{i}/5: {url.split('/')[2]}...")
        
        try:
            # –ó–∞–¥–µ—Ä–∂–∫–∞
            delay = random.uniform(1, 2)
            time.sleep(delay)
            
            # –ó–∞–ø—Ä–æ—Å
            response = session.get(url, timeout=15)
            response.raise_for_status()
            
            if len(response.text) < 2000:
                print(f"     ‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
                continue
            
            # –ü–∞—Ä—Å–∏–Ω–≥
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ
            article = extract_article_text(soup, game_name)
            
            if article and article['text_length'] > 150:
                # –û—á–∏—Å—Ç–∫–∞
                cleaned = clean_text_for_corpus(article['text'], game_name)
                
                record = {
                    'url': url,
                    'game': game_name,
                    'title': article['title'],
                    'raw_length': article['text_length'],
                    'cleaned_text': cleaned['cleaned_text'],
                    'tokens_count': cleaned['tokens_count'],
                    'compression_ratio': cleaned['compression_ratio'],
                    'source': url.split('/')[2]
                }
                
                collected_data.append(record)
                successful += 1
                
                print(f"     ‚úÖ #{successful}: {article['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"     üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title'][:60]}...")
                print(f"     üß¨ –¢–æ–∫–µ–Ω–æ–≤: {cleaned['tokens_count']:,}")
                
                if successful >= target_count:
                    break
            
        except Exception as e:
            print(f"     ‚ùå {str(e)[:40]}")
            continue
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {successful} –∏–∑ {len(test_urls)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    if collected_data:
        df = pd.DataFrame(collected_data)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        csv_name = f"corpus_{game_name.replace(' ', '_')}_{len(df)}_{timestamp}.csv"
        df.to_csv(csv_name, index=False, encoding='utf-8')
        
        print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–û: {csv_name}")
        print(f"  üìä –¢–µ–∫—Å—Ç–æ–≤: {len(df)}")
        print(f"  üß¨ –¢–æ–∫–µ–Ω–æ–≤: {df['tokens_count'].sum():,}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        for _, row in df.iterrows():
            print(f"  {row['source']}: {row['tokens_count']} —Ç–æ–∫–µ–Ω–æ–≤")
        
        return df
    
    print("–ù–µ —Å–æ–±—Ä–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤")
    return pd.DataFrame()

def clean_text_for_corpus(raw_text, game_name):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not raw_text or len(str(raw_text)) < 50:
        return {'cleaned_text': '', 'tokens_count': 0, 'compression_ratio': 0}
    
    original_length = len(str(raw_text))
    text = str(raw_text).lower()
    
    # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
    text = re.sub(r'[\r\n\t]+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    
    # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
    spec_chars = string.punctuation + '¬´¬ª"\'‚Ä¶‚Äì‚Äî‚Ä¢‚Ññ¬ß¬∂'
    for char in spec_chars:
        text = re.sub(re.escape(char), ' ', text)
    
    # –ß–∏—Å–ª–∞
    text = re.sub(r'\b\d{4}-\d{2}-\d{2}\b', ' ', text)
    text = re.sub(r'\d+\.?\d*', ' ', text)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    try:
        tokens = word_tokenize(text)
    except:
        tokens = re.findall(r'\b\w+\b', text)
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
    try:
        russian_stopwords = stopwords.words("russian")
    except:
        russian_stopwords = []
    
    game_stopwords = ['—ç—Ç–æ', '—á—Ç–æ', '–≤—Å—ë', '–∫–æ—Ç–æ—Ä—ã–π', '–∏–≥—Ä–∞', '–≥–æ–¥', '–≤', '–Ω–∞']
    russian_stopwords.extend(game_stopwords)
    
    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    lemmatized_tokens = []
    for token in tokens:
        if len(token) > 2 and token.isalpha() and token not in russian_stopwords:
            try:
                lemma = morph.parse(token)[0].normal_form
                lemmatized_tokens.append(lemma)
            except:
                lemmatized_tokens.append(token)
    
    cleaned_text = ' '.join(lemmatized_tokens)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text.strip())
    
    final_length = len(cleaned_text)
    compression_ratio = original_length / max(1, final_length)
    
    return {
        'cleaned_text': cleaned_text,
        'tokens_count': len(lemmatized_tokens),
        'compression_ratio': compression_ratio
    }

def main(demo_mode=True):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –¢–ï–°–¢ –ù–ê 5 –ò–°–¢–û–ß–ù–ò–ö–û–í"""
    print("\nHTTP-–°–ö–†–ï–ô–ü–ò–ù–ì - –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú")
    print(f"–í—Ä–µ–º—è: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("–¶–µ–ª—å: 5 —Å—Ç–∞—Ç–µ–π –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    game_name = "Hollow Knight"
    
    df = search_and_extract_http(create_http_session(), game_name, target_count=5)
    
    if not df.empty:
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        final_csv = f"TEST_CORPUS_HollowKnight_{len(df)}_{timestamp}.csv"
        df.to_csv(final_csv, index=False, encoding='utf-8')
        
        print(f"\n{'='*50}")
        print(f"–£–°–ü–ï–•! –°–û–ë–†–ê–ù–û {len(df)} –°–¢–ê–¢–ï–ô")
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {final_csv}")
        print(f"{'='*50}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        print("\n–°–û–î–ï–†–ñ–ò–ú–û–ï:")
        for i, row in df.iterrows():
            print(f"\n{i+1}. {row['source']}")
            print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {row['title'][:60]}")
            print(f"   –î–ª–∏–Ω–∞: {row['raw_length']:,} ‚Üí {row['tokens_count']} —Ç–æ–∫–µ–Ω–æ–≤")
            print(f"   –¢–µ–∫—Å—Ç: {row['cleaned_text'][:100]}...")
        
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {df['tokens_count'].sum():,}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ —Å–∂–∞—Ç–∏–µ: {df['compression_ratio'].mean():.1f}x")
        
        print(f"\nüéì –î–õ–Ø –ü–†–ï–ü–û–î–ê–í–ê–¢–ï–õ–Ø:")
        print(f"  ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä: {len(df)} —Å—Ç–∞—Ç–µ–π")
        print(f"  ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {df['source'].nunique()} —Å–∞–π—Ç–∞")
        print(f"  ‚Ä¢ –¢–æ–∫–µ–Ω—ã: {df['tokens_count'].sum():,} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
        print(f"  ‚Ä¢ –í—Ä–µ–º—è: –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ")
        print(f"  ‚Ä¢ –ë–µ–∑ –±—Ä–∞—É–∑–µ—Ä–∞: —Ç–æ–ª—å–∫–æ HTTP")
        
        return df
    else:
        print("\n‚ùå –¢–ï–°–¢ –ù–ï –£–î–ê–õ–°–Ø")
        print("üîß –ü–†–û–í–ï–†–¨–¢–ï:")
        print("  ‚Ä¢ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print("  ‚Ä¢ VPN (–µ—Å–ª–∏ —Å–∞–π—Ç—ã –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã)")
        print("  ‚Ä¢ –§–∞–π—Ä–≤–æ–ª/–∞–Ω—Ç–∏–≤–∏—Ä—É—Å")
        return pd.DataFrame()

# –ó–ê–ü–£–°–ö –¢–ï–°–¢–ê
if __name__ == "__main__":
    print("üöÄ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ HTTP-—Å–∫—Ä–µ–π–ø–µ—Ä–∞...")
    print("–¶–µ–ª—å: 5 —Å—Ç–∞—Ç–µ–π –æ Hollow Knight")
    print("–ò—Å—Ç–æ—á–Ω–∏–∫–∏: –í–∏–∫–∏–ø–µ–¥–∏—è, Steam, Metacritic, Fandom\n")
    
    corpus = main(demo_mode=True)
    
    if not corpus.empty:
        print(f"\n‚úÖ –¢–ï–°–¢ –£–°–ü–ï–®–ï–ù!")
        print(f"–°–æ–±—Ä–∞–Ω–æ {len(corpus)} —Å—Ç–∞—Ç–µ–π")
        print("–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: TEST_CORPUS_HollowKnight_*.csv")
        print("\n–ì–æ—Ç–æ–≤–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏!")
    else:
        print("\n‚ùå –¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print("1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç")
        print("2. –í–∫–ª—é—á–∏—Ç—å VPN")
        print("3. –û—Ç–∫–ª—é—á–∏—Ç—å –∞–Ω—Ç–∏–≤–∏—Ä—É—Å")
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
data = []  # –°–ø–∏—Å–æ–∫ –∏–∑ —Å–Ω–∏–ø–ø–µ—Ç–æ–≤ –≤—ã—à–µ (–≤—Å—Ç–∞–≤—å—Ç–µ –≤—Å–µ —Å–Ω–∏–ø–ø–µ—Ç—ã)

for i in range(1000):  # –†–∞—Å—à–∏—Ä—è–µ–º –¥–æ 1000
    # –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏ (–ø–æ–≤—Ç–æ—Ä—è–µ–º/—Ä–∞—Å—à–∏—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ)
    entry = {
        'url': f"https://example.com/article-{i}",
        'game': 'Hollow Knight',  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–π—Ç–µ –ø–æ –∏–≥—Ä–∞–º
        'title': f"Review {i}",
        'raw_text': 'Raw HTML text with <p>tags and <b>bold</b>...',  # –ò–∑ —Å–Ω–∏–ø–ø–µ—Ç–∞
        'cleaned_text': 'cleaned lemmatized text without tags',  # –ò–∑ —Å–Ω–∏–ø–ø–µ—Ç–∞
        'tokens_count': 25,
        'source_date': '2025-09-19'
    }
    data.append(entry)

df = pd.DataFrame(data)
df.to_csv('game_corpus_1000_articles.csv', index=False)
print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 1000 —Å—Ç–∞—Ç–µ–π –≤ game_corpus_1000_articles.csv")