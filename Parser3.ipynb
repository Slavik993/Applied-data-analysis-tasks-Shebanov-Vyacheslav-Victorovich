{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8dcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# LIBRARIES\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import inspect\n",
    "\n",
    "print(\"🔧 Initializing libraries...\")\n",
    "\n",
    "# PATCH FOR PYMORPHY2 (FIX AttributeError: 'inspect' has no attribute 'getargspec')\n",
    "def patch_pymorphy2():\n",
    "    def getargspec_patch(func):\n",
    "        try:\n",
    "            args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)\n",
    "            return args, varargs, varkw, defaults\n",
    "        except Exception:\n",
    "            return [], None, None, None\n",
    "    inspect.getargspec = getargspec_patch\n",
    "\n",
    "patch_pymorphy2()\n",
    "\n",
    "# NLTK SETUP\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK resources...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "print(\"  ✅ NLTK ready\")\n",
    "\n",
    "# PYMORPHY2 SETUP\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(\"  ✅ Pymorphy2 ready\")\n",
    "\n",
    "# SPACY SETUP\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    print(\"Downloading spaCy model: python -m spacy download en_core_web_sm\")\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "print(\"  ✅ spaCy ready\")\n",
    "\n",
    "# STOP WORDS\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('russian'))\n",
    "print(\"  ✅ Stop words loaded\")\n",
    "\n",
    "# LIST OF GAMES\n",
    "GAMES = [\n",
    "    \"Hollow Knight\", \"Hollow Knight Silksong\", \"Platypus\",\n",
    "    \"Hard Truck Apocalypse\", \"No Man's Sky\", \"Moonlighter\", \"Minecraft\"\n",
    "]\n",
    "\n",
    "# PLACEHOLDER URL LIST (for demo, ~27 URLs)\n",
    "BASE_URLS = [\n",
    "    # Hollow Knight\n",
    "    \"https://en.wikipedia.org/wiki/Hollow_Knight\",\n",
    "    \"https://hollowknight.fandom.com/wiki/Hollow_Knight\",\n",
    "    \"https://store.steampowered.com/app/367520/Hollow_Knight\",\n",
    "    \"https://www.metacritic.com/game/pc/hollow-knight\",\n",
    "    \"https://www.ign.com/articles/2018/06/22/hollow-knight-review\",\n",
    "    \"https://www.pcgamer.com/hollow-knight-review\",\n",
    "    \"https://www.gamespot.com/reviews/hollow-knight-review-an-exceptional-adventure/1900-6416972\",\n",
    "    # Hollow Knight Silksong\n",
    "    \"https://hollowknight.fandom.com/wiki/Silksong\",\n",
    "    \"https://www.pcgamer.com/games/action/hollow-knight-silksong-review\",\n",
    "    \"https://www.metacritic.com/game/pc/hollow-knight-silksong\",\n",
    "    # Platypus\n",
    "    \"https://en.wikipedia.org/wiki/Platypus_(video_game)\",\n",
    "    \"https://store.steampowered.com/app/307340/Platypus\",\n",
    "    \"https://www.mobygames.com/game/10766/platypus\",\n",
    "    # Hard Truck Apocalypse\n",
    "    \"https://en.wikipedia.org/wiki/Hard_Truck_Apocalypse\",\n",
    "    \"https://store.steampowered.com/app/307320/Hard_Truck_Apocalypse\",\n",
    "    \"https://www.mobygames.com/game/14994/hard-truck-apocalypse\",\n",
    "    # No Man's Sky\n",
    "    \"https://en.wikipedia.org/wiki/No_Man%27s_Sky\",\n",
    "    \"https://www.nomanssky.com/news\",\n",
    "    \"https://store.steampowered.com/app/275850/No_Mans_Sky\",\n",
    "    \"https://www.ign.com/articles/no-mans-sky-review\",\n",
    "    # Moonlighter\n",
    "    \"https://en.wikipedia.org/wiki/Moonlighter_(video_game)\",\n",
    "    \"https://store.steampowered.com/app/606150/Moonlighter\",\n",
    "    \"https://www.ign.com/games/moonlighter\",\n",
    "    # Minecraft\n",
    "    \"https://en.wikipedia.org/wiki/Minecraft\",\n",
    "    \"https://www.minecraft.net/en-us\",\n",
    "    \"https://minecraft.fandom.com/wiki/Minecraft_Wiki\",\n",
    "    \"https://www.ign.com/games/minecraft\",\n",
    "] * 40  # ~1000 URLs\n",
    "\n",
    "# FOR 1000+ URLs VIA GOOGLE SEARCH API\n",
    "# Install: pip install google-api-python-client\n",
    "# Get API_KEY and CSE_ID: https://console.cloud.google.com, https://programmablesearchengine.google.com\n",
    "# Uncomment below:\n",
    "\"\"\"\n",
    "from googleapiclient.discovery import build\n",
    "def get_search_urls(query, api_key, cse_id, num_results=10):\n",
    "    try:\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        result = service.cse().list(q=query, cx=cse_id, num=num_results).execute()\n",
    "        return [item['link'] for item in result.get('items', [])]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Google Search API error: {e}\")\n",
    "        return []\n",
    "API_KEY = \"YOUR_GOOGLE_API_KEY\"\n",
    "CSE_ID = \"YOUR_CUSTOM_SEARCH_ENGINE_ID\"\n",
    "urls = []\n",
    "for game in GAMES:\n",
    "    urls.extend(get_search_urls(game + \" game review\", API_KEY, CSE_ID, num_results=30))\n",
    "unique_urls = list(dict.fromkeys(urls))[:1000]\n",
    "\"\"\"\n",
    "unique_urls = list(dict.fromkeys(BASE_URLS))[:1000]\n",
    "print(f\"\\n📚 Unique URLs for scraping: {len(unique_urls)}\")\n",
    "\n",
    "# HTTP SESSION\n",
    "def create_http_session():\n",
    "    session = requests.Session()\n",
    "    ua = UserAgent()\n",
    "    session.headers.update({'User-Agent': ua.random})\n",
    "    return session\n",
    "\n",
    "# PARSE PAGE\n",
    "def parse_page(url, session, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.title.string.strip() if soup.title else 'No Title'\n",
    "            content = soup.find_all(['p', 'div', 'article'])\n",
    "            raw_text = ' '.join([elem.get_text(strip=True) for elem in content if elem.get_text(strip=True)])\n",
    "            if not raw_text or len(raw_text) < 100:\n",
    "                return None\n",
    "            date = datetime.now().strftime('%Y-%m-%d')\n",
    "            return {'title': title, 'raw_text': raw_text[:5000], 'date': date}\n",
    "        except (requests.RequestException, Exception) as e:\n",
    "            print(f\"⚠ Error scraping {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "    return None\n",
    "\n",
    "# CLEAN AND LEMMATIZE TEXT\n",
    "def clean_text(raw_text, is_russian=False):\n",
    "    text = re.sub(r'<[^>]+>', ' ', raw_text)\n",
    "    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)\n",
    "    text = re.sub(r'\\d+\\.?\\d*', ' ', text)\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    if is_russian:\n",
    "        tokens = [morph.parse(token)[0].normal_form for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    else:\n",
    "        doc = nlp(' '.join(tokens))\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens[:200]), len(tokens[:200])\n",
    "\n",
    "# DETERMINE GAME\n",
    "def get_game_from_url(url, title):\n",
    "    for game in GAMES:\n",
    "        if game.lower().replace(' ', '') in url.lower().replace(' ', '') or game.lower().replace(' ', '') in title.lower().replace(' ', ''):\n",
    "            return game\n",
    "    return \"Unknown\"\n",
    "\n",
    "# MAIN PROCESS\n",
    "def main():\n",
    "    print(f\"\\n🚀 Starting scraping {len(unique_urls)} articles...\")\n",
    "    session = create_http_session()\n",
    "    corpus_data = []\n",
    "    parsed_urls = set()\n",
    "    doc_id = 1\n",
    "\n",
    "    random.shuffle(unique_urls)\n",
    "    for url in unique_urls:\n",
    "        if doc_id > 1000:\n",
    "            break\n",
    "        if url in parsed_urls:\n",
    "            continue\n",
    "        print(f\"  🔍 Scraping {url}...\")\n",
    "        parsed = parse_page(url, session)\n",
    "        if parsed and parsed['raw_text']:\n",
    "            is_russian = 'ru.' in url or 'russian' in parsed['raw_text'].lower()\n",
    "            cleaned_text, token_count = clean_text(parsed['raw_text'], is_russian)\n",
    "            if token_count > 10:\n",
    "                game = get_game_from_url(url, parsed['title'])\n",
    "                corpus_data.append({\n",
    "                    'doc_id': doc_id,\n",
    "                    'game': game,\n",
    "                    'title': parsed['title'][:100],\n",
    "                    'url': url,\n",
    "                    'raw_text': parsed['raw_text'][:1000],\n",
    "                    'cleaned_text': cleaned_text,\n",
    "                    'tokens_count': token_count,\n",
    "                    'date': parsed['date']\n",
    "                })\n",
    "                parsed_urls.add(url)\n",
    "                print(f\"    ✅ Document {doc_id} added: {token_count} tokens ({game})\")\n",
    "                doc_id += 1\n",
    "            else:\n",
    "                print(f\"    ⚠ Skipped: too little text\")\n",
    "        else:\n",
    "            print(f\"    ⚠ Skipped: scraping error\")\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "    # SAVE CORPUS TO TXT\n",
    "    print(f\"\\n💾 Creating corpus: game_corpus_1000.txt\")\n",
    "    with open('game_corpus_1000.txt', 'w', encoding='utf-8') as f:\n",
    "        for doc in corpus_data:\n",
    "            f.write(f\"=== Document {doc['doc_id']} | {doc['game']} | {doc['title']} | {doc['url']} ===\\n\")\n",
    "            f.write(f\"Tokens: {doc['tokens_count']} | Date: {doc['date']}\\n\")\n",
    "            f.write(f\"{doc['cleaned_text']}\\n---\\n\")\n",
    "    print(f\"  ✅ TXT corpus saved: {len(corpus_data)} documents\")\n",
    "\n",
    "    # SAVE TO CSV\n",
    "    df = pd.DataFrame(corpus_data)\n",
    "    df.to_csv('game_corpus_1000.csv', index=False, encoding='utf-8')\n",
    "    print(f\"  ✅ CSV saved: game_corpus_1000.csv\")\n",
    "\n",
    "    # STATISTICS\n",
    "    total_tokens = sum(doc['tokens_count'] for doc in corpus_data)\n",
    "    print(f\"\\n📊 Corpus statistics:\")\n",
    "    print(f\"  Documents: {len(corpus_data)}\")\n",
    "    print(f\"  Tokens: {total_tokens}\")\n",
    "    print(f\"  Avg tokens/document: {total_tokens / len(corpus_data):.2f}\" if corpus_data else \"  No data\")\n",
    "    print(f\"  Unique URLs: {len(parsed_urls)}\")\n",
    "    print(f\"\\n🎉 Corpus ready for NLP (LDA, clustering)!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
