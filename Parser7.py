import warnings
warnings.filterwarnings("ignore")

# –ë–ò–ë–õ–ò–û–¢–ï–ö–ò
import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import time
import random
import re
import string
import pandas as pd
import os
from urllib.parse import urljoin, urlparse, quote_plus
from datetime import datetime
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
import pymorphy2
import inspect

print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫...")

# –ü–ê–¢–ß –î–õ–Ø PYMORPHY2
def patch_pymorphy2():
    def getargspec_patch(func):
        try:
            args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)
            return args, varargs, varkw, defaults
        except Exception:
            return [], None, None, None
    inspect.getargspec = getargspec_patch

patch_pymorphy2()

# –ù–ê–°–¢–†–û–ô–ö–ê NLTK
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK...")
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    nltk.download('stopwords', quiet=True)
print("  ‚úÖ NLTK –≥–æ—Ç–æ–≤")

# –ù–ê–°–¢–†–û–ô–ö–ê PYMORPHY2
morph = pymorphy2.MorphAnalyzer()
print("  ‚úÖ Pymorphy2 –≥–æ—Ç–æ–≤")

# –°–¢–û–ü-–°–õ–û–í–ê
stop_words = set(stopwords.words('russian') + stopwords.words('english'))
print("  ‚úÖ –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ò–ì–† (50+ –∏–≥—Ä)
GAMES = [
    # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ
    "Atomic Heart", "Metro Exodus", "Escape from Tarkov", "Pathfinder",
    "S.T.A.L.K.E.R.", "War Thunder", "World of Tanks", "Crossout",
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ
    "Hollow Knight", "Hollow Knight Silksong", "Platypus",
    "Hard Truck Apocalypse", "No Man's Sky", "Moonlighter", "Minecraft",
    # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏–Ω–¥–∏
    "Celeste", "Hades", "Stardew Valley", "Undertale", "Terraria",
    "Dead Cells", "Ori and the Blind Forest", "Cuphead", "Shovel Knight",
    "Subnautica", "The Binding of Isaac", "Risk of Rain 2", "Slay the Spire",
    # AAA –∏–≥—Ä—ã
    "The Witcher 3", "Elden Ring", "Dark Souls", "Sekiro", "Bloodborne",
    "God of War", "Red Dead Redemption 2", "Cyberpunk 2077", "Skyrim",
    "Grand Theft Auto V", "The Last of Us", "Horizon Zero Dawn",
    # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏
    "Civilization VI", "StarCraft II", "Age of Empires IV", "Total War",
    "XCOM 2", "Crusader Kings III", "Cities Skylines",
    # –®—É—Ç–µ—Ä—ã
    "Counter-Strike 2", "Valorant", "Apex Legends", "Overwatch 2",
    "Call of Duty", "Battlefield", "Destiny 2", "Halo Infinite",
    # MMORPG/–û–Ω–ª–∞–π–Ω
    "World of Warcraft", "Final Fantasy XIV", "Guild Wars 2", "Dota 2", "League of Legends"
]

# –§–£–ù–ö–¶–ò–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –ê–í–¢–û–†–ê
def extract_author(soup, url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≤—Ç–æ—Ä–∞ —Å—Ç–∞—Ç—å–∏ –∏–∑ HTML"""
    # –ü–æ–ø—ã—Ç–∫–∞ 1: –ú–µ—Ç–∞-—Ç–µ–≥–∏
    meta_tags = [
        soup.find('meta', {'name': 'author'}),
        soup.find('meta', {'property': 'article:author'}),
        soup.find('meta', {'name': 'article:author'}),
        soup.find('meta', {'property': 'author'}),
    ]
    
    for tag in meta_tags:
        if tag and tag.get('content'):
            return tag.get('content').strip()
    
    # –ü–æ–ø—ã—Ç–∫–∞ 2: –ö–ª–∞—Å—Å –∞–≤—Ç–æ—Ä–∞
    author_classes = [
        'author', 'author-name', 'post-author', 'article-author',
        'byline', 'by-author', 'entry-author', 'writer',
        '–∞–≤—Ç–æ—Ä', 'author_name', 'article__author'
    ]
    
    for class_name in author_classes:
        author_tag = soup.find(class_=re.compile(class_name, re.I))
        if author_tag:
            text = author_tag.get_text(strip=True)
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
            text = re.sub(r'^(by|–∞–≤—Ç–æ—Ä|written by|posted by)[\s:]+', '', text, flags=re.I)
            if text and len(text) < 100:
                return text
    
    # –ü–æ–ø—ã—Ç–∫–∞ 3: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
    selectors = [
        'a[rel="author"]',
        'span[itemprop="author"]',
        'span[itemprop="name"]',
        '[class*="author"] a',
        '.author-info a',
    ]
    
    for selector in selectors:
        author_tag = soup.select_one(selector)
        if author_tag:
            text = author_tag.get_text(strip=True)
            if text and len(text) < 100:
                return text
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞
    domain = urlparse(url).netloc
    return domain.replace('www.', '').split('.')[0].capitalize()

# –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê –ß–ï–†–ï–ó YANDEX (–¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
def search_yandex(query, num_results=15):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ Yandex"""
    try:
        search_url = f"https://yandex.ru/search/?text={quote_plus(query)}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'ru-RU,ru;q=0.9'
        }
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        links = []
        for result in soup.find_all('a', class_=re.compile('Link.*Organic'), limit=num_results):
            href = result.get('href')
            if href and href.startswith('http'):
                links.append(href)

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        if not links:
            for result in soup.find_all('li', class_=re.compile('serp-item'), limit=num_results):
                link = result.find('a')
                if link and link.get('href'):
                    href = link.get('href')
                    if href.startswith('http'):
                        links.append(href)

        print(f"    üîç Yandex: –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –¥–ª—è '{query}'")
        return links
    except Exception as e:
        print(f"    ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Yandex: {e}")
        return []

# –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê –ß–ï–†–ï–ó DUCKDUCKGO (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π)
def search_duckduckgo(query, num_results=15):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ DuckDuckGo"""
    try:
        search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
        }
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        links = []
        for result in soup.find_all('a', class_='result__a', limit=num_results):
            href = result.get('href')
            if href and href.startswith('http'):
                links.append(href)

        print(f"    üîç DuckDuckGo: –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –¥–ª—è '{query}'")
        return links
    except Exception as e:
        print(f"    ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}")
        return []

# –°–ë–û–† –ë–ê–ó–û–í–´–• URL (—Å—Ç–∞—Ç–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ + —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ)
def get_base_urls():
    """–ë–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ + –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ)"""
    base_urls = []

    # –®–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
    ru_templates = [
        "https://stopgame.ru/search?q={game}",
        "https://dtf.ru/search?q={game}",
        "https://igromania.ru/search/?q={game}",
        "https://www.playground.ru/search?q={game}",
        "https://www.kanobu.ru/search/?q={game}",
        "https://vgtimes.ru/search/?q={game}",
        "https://gamemag.ru/search?q={game}",
        "https://cybersport.ru/search?q={game}",
        "https://habr.com/ru/search/?q={game}",
        "https://vc.ru/search?q={game}",
        "https://stopgame.ru/games/{game}",
        "https://dtf.ru/games/{game}",
    ]

    # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    en_templates = [
        "https://ru.wikipedia.org/wiki/{game}",
        "https://store.steampowered.com/search/?term={game}",
        "https://www.metacritic.com/search/{game}",
        "https://www.ign.com/games/{game}",
        "https://www.pcgamer.com/search/?searchTerm={game}",
        "https://www.gamespot.com/search/?q={game}",
    ]

    all_templates = ru_templates + en_templates

    # –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú: –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –∏–≥—Ä—ã
    for game in GAMES:
        game_slug = game.lower().replace(' ', '-').replace("'", '')
        game_encoded = quote_plus(game)
        
        for template in all_templates:
            if '{game}' in template:
                if 'search' in template or '?' in template:
                    url = template.format(game=game_encoded)
                else:
                    url = template.format(game=game_slug)
                base_urls.append(url)

    return base_urls

# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –°–ë–û–†–ê URL
def collect_urls(target_count=2500):
    """–°–æ–±–∏—Ä–∞–µ—Ç URLs –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è target_count"""
    print(f"\nüì° –°–±–æ—Ä URL (—Ü–µ–ª—å: {target_count})...")
    all_urls = set()

    # 1. –ë–∞–∑–æ–≤—ã–µ URL
    print("\n1Ô∏è‚É£ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö URL...")
    base_urls = get_base_urls()
    all_urls.update(base_urls)
    print(f"  ‚úÖ –ë–∞–∑–æ–≤—ã—Ö URL: {len(all_urls)}")

    # 2. –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex –∏ DuckDuckGo
    print("\n2Ô∏è‚É£ –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex & DuckDuckGo...")
    search_queries = []

    # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∏–≥—Ä (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ) - –£–í–ï–õ–ò–ß–ï–ù–û
    for game in GAMES[:50]:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –∏–≥—Ä
        search_queries.extend([
            f"{game} –æ–±–∑–æ—Ä –∏–≥—Ä—ã",
            f"{game} –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ",
            f"{game} —Ä–µ—Ü–µ–Ω–∑–∏—è",
            f"{game} —Å—Ç–∞—Ç—å—è",
        ])

    # –û–±—â–∏–µ –∏–≥—Ä–æ–≤—ã–µ —Ç–µ–º—ã (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ) - –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö
    general_topics = [
        "–æ–±–∑–æ—Ä—ã –∏–Ω–¥–∏ –∏–≥—Ä", "–ª—É—á—à–∏–µ –∏–≥—Ä—ã 2024", "–∏–≥—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–≥—Ä", "–∫–∏–±–µ—Ä—Å–ø–æ—Ä—Ç", "–∏–≥—Ä–æ–≤–∞—è –∫—É–ª—å—Ç—É—Ä–∞",
        "—Ä–µ—Ç—Ä–æ –∏–≥—Ä—ã", "–≥–µ–π–º–¥–∏–∑–∞–π–Ω", "–∏—Å—Ç–æ—Ä–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä",
        "–∏–≥—Ä–æ–≤–∞—è –∏–Ω–¥—É—Å—Ç—Ä–∏—è", "–∏–≥—Ä–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞", "–†–ü–ì –∏–≥—Ä—ã",
        "—ç–∫—à–µ–Ω –∏–≥—Ä—ã", "—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏", "—Å–∏–º—É–ª—è—Ç–æ—Ä—ã",
        "–∏–≥—Ä–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏", "–∏–≥—Ä–æ–≤–∞—è –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∞", "–æ–±–∑–æ—Ä—ã –Ω–æ–≤–∏–Ω–æ–∫",
        "–ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∏–≥—Ä", "–≥–∞–π–¥—ã –ø–æ –∏–≥—Ä–∞–º", "–∏–≥—Ä–æ–≤—ã–µ —Å–æ–≤–µ—Ç—ã",
        "–∏–≥—Ä–æ–≤—ã–µ —Ä–µ—Ü–µ–Ω–∑–∏–∏", "–ª—É—á—à–∏–µ –†–ü–ì", "–ª—É—á—à–∏–µ –∏–Ω–¥–∏",
        "–∏–≥—Ä—ã 2025", "–Ω–æ–≤—ã–µ –∏–≥—Ä—ã", "–∞–Ω–æ–Ω—Å—ã –∏–≥—Ä",
        "–∏–≥—Ä–æ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã", "–±—É–¥—É—â–µ–µ –∏–≥—Ä", "VR –∏–≥—Ä—ã",
        "–º–æ–±–∏–ª—å–Ω—ã–µ –∏–≥—Ä—ã", "–∫–æ–Ω—Å–æ–ª—å–Ω—ã–µ –∏–≥—Ä—ã", "PC –∏–≥—Ä—ã"
    ]
    search_queries.extend(general_topics)

    # –ü–æ–∏—Å–∫ - –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
    for i, query in enumerate(search_queries[:100]):  # –ë–´–õ–û 60, –°–¢–ê–õ–û 100
        if len(all_urls) >= target_count:
            break

        print(f"  üîç –ó–∞–ø—Ä–æ—Å {i+1}/100: '{query}'")

        # Yandex (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        if random.random() > 0.3:  # 70% –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Yandex
            yandex_links = search_yandex(query, num_results=20)  # –£–í–ï–õ–ò–ß–ï–ù–û —Å 15 –¥–æ 20
            all_urls.update(yandex_links)
            time.sleep(random.uniform(2, 4))

        # DuckDuckGo (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ)
        if len(all_urls) < target_count:
            ddg_links = search_duckduckgo(query, num_results=20)  # –£–í–ï–õ–ò–ß–ï–ù–û —Å 15 –¥–æ 20
            all_urls.update(ddg_links)
            time.sleep(random.uniform(2, 4))

        if i % 10 == 0:
            print(f"  üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ URL: {len(all_urls)}")

    # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
    filtered_urls = []
    blocked_domains = [
        'youtube.com', 'twitter.com', 'facebook.com', 'instagram.com',
        'reddit.com', 'discord.com', 'tiktok.com', 'pinterest.com',
        'vk.com', 'ok.ru', 't.me'  # –°–æ—Ü. —Å–µ—Ç–∏
    ]

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–º –¥–æ–º–µ–Ω–∞–º
    ru_priority_domains = [
        'stopgame.ru', 'dtf.ru', 'igromania.ru', 'playground.ru',
        'kanobu.ru', 'vgtimes.ru', 'gamemag.ru', 'cybersport.ru',
        'habr.com', 'vc.ru'
    ]

    ru_urls = []
    other_urls = []

    for url in all_urls:
        domain = urlparse(url).netloc
        if any(blocked in domain for blocked in blocked_domains):
            continue
        
        if any(ru_domain in domain for ru_domain in ru_priority_domains):
            ru_urls.append(url)
        else:
            other_urls.append(url)

    # –°–Ω–∞—á–∞–ª–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ, –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
    filtered_urls = ru_urls + other_urls

    print(f"\n‚úÖ –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL: {len(filtered_urls)}")
    print(f"   üìå –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö —Å–∞–π—Ç–æ–≤: {len(ru_urls)}")
    print(f"   üåê –û—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤: {len(other_urls)}")
    
    return filtered_urls[:target_count]

# HTTP SESSION
def create_http_session():
    session = requests.Session()
    ua = UserAgent()
    session.headers.update({
        'User-Agent': ua.random,
        'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
    })
    return session

# –ü–ê–†–°–ò–ù–ì –°–¢–†–ê–ù–ò–¶–´ - –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–ï–ó –°–õ–ò–ü–ê–ù–ò–Ø –°–õ–û–í
def parse_page(url, session, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = session.get(url, timeout=15)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            if response.encoding.lower() in ['iso-8859-1', 'windows-1251']:
                response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.content, 'html.parser')

            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']):
                tag.decompose()

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.title.string.strip() if soup.title else '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'
            
            # –ê–≤—Ç–æ—Ä
            author = extract_author(soup, url)
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç - –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            content_parts = []
            for elem in soup.find_all(['p', 'article', 'div', 'section'], limit=150):
                text = elem.get_text(separator=' ', strip=True)  # separator=' ' - –∫–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ!
                if text:
                    content_parts.append(text)
            
            raw_text = ' '.join(content_parts)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–±–µ–ª –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏

            if not raw_text or len(raw_text) < 200:
                return None

            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–ø–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å)
            date = datetime.now().strftime('%Y-%m-%d')
            date_meta = soup.find('meta', {'property': 'article:published_time'}) or \
                       soup.find('meta', {'name': 'date'}) or \
                       soup.find('time')
            
            if date_meta:
                date_str = date_meta.get('content') or date_meta.get('datetime') or date_meta.get_text()
                try:
                    date = datetime.fromisoformat(date_str.split('T')[0]).strftime('%Y-%m-%d')
                except:
                    pass

            return {
                'title': title,
                'author': author,
                'raw_text': raw_text[:10000],
                'date': date
            }
        except (requests.RequestException, Exception) as e:
            if attempt < max_retries - 1:
                time.sleep(random.uniform(1, 3))
    return None

# –û–ß–ò–°–¢–ö–ê –ò –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê
def clean_text(raw_text):
    """–û—á–∏—Å—Ç–∫–∞ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    # –£–¥–∞–ª–µ–Ω–∏–µ HTML
    text = re.sub(r'<[^>]+>', ' ', raw_text)
    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
    # –£–¥–∞–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª
    text = re.sub(r'\d+\.?\d*', ' ', text)
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'[{}]'.format(string.punctuation + '¬´¬ª‚Äî‚Äì‚Ä¶""‚Äû'), ' ', text)
    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'\s+', ' ', text.strip())

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = word_tokenize(text.lower())
    
    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å pymorphy2
    lemmatized_tokens = []
    for token in tokens:
        if token.isalpha() and len(token) > 2 and token not in stop_words:
            parsed = morph.parse(token)[0]
            lemmatized_tokens.append(parsed.normal_form)

    return ' '.join(lemmatized_tokens[:400]), len(lemmatized_tokens[:400])

# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ò–ì–†–´
def get_game_from_url(url, title):
    text_to_check = (url + ' ' + title).lower()
    for game in GAMES:
        if game.lower().replace(' ', '') in text_to_check.replace(' ', '').replace('-', ''):
            return game
    return "–ò–≥—Ä—ã (–æ–±—â–µ–µ)"

# –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–°
def main():
    print("\n" + "="*70)
    print("üéÆ –°–ë–û–†–©–ò–ö –¢–ï–ö–°–¢–û–í–û–ì–û –ö–û–†–ü–£–°–ê - 1000+ –î–û–ö–£–ú–ï–ù–¢–û–í (–†–£–°–°–ö–û–Ø–ó–´–ß–ù–´–ô)")
    print("="*70)

    # –°–±–æ—Ä URL
    target_docs = 1000
    target_urls = 2500  # –£–í–ï–õ–ò–ß–ï–ù–û: –±—ã–ª–æ 1800, —Å—Ç–∞–ª–æ 2500 –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ 1000+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    unique_urls = collect_urls(target_count=target_urls)

    print(f"\nüöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ {len(unique_urls)} URL (—Ü–µ–ª—å: {target_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
    session = create_http_session()
    corpus_data = []
    parsed_urls = set()
    doc_id = 1
    failed_count = 0
    too_short_count = 0

    random.shuffle(unique_urls)

    for i, url in enumerate(unique_urls):
        if doc_id > target_docs:
            break

        if url in parsed_urls:
            continue

        if i % 50 == 0:
            print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(unique_urls)} URL | {doc_id-1} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | –û—à–∏–±–æ–∫: {failed_count} | –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞: {too_short_count}")

        print(f"  üîç [{doc_id}] {url[:80]}...")
        parsed = parse_page(url, session)

        if parsed and parsed['raw_text']:
            cleaned_text, token_count = clean_text(parsed['raw_text'])

            if token_count > 30:  # –ú–∏–Ω–∏–º—É–º 30 —Ç–æ–∫–µ–Ω–æ–≤
                game = get_game_from_url(url, parsed['title'])
                corpus_data.append({
                    'doc_id': doc_id,
                    'game': game,
                    'title': parsed['title'][:200],
                    'author': parsed['author'],
                    'url': url,
                    'raw_text': parsed['raw_text'][:2000],
                    'cleaned_text': cleaned_text,
                    'tokens_count': token_count,
                    'date': parsed['date']
                })
                parsed_urls.add(url)
                print(f"    ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {doc_id}: {token_count} —Ç–æ–∫–µ–Ω–æ–≤ | –ê–≤—Ç–æ—Ä: {parsed['author']} | {game}")
                doc_id += 1
            else:
                too_short_count += 1
                print(f"    ‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ ({token_count} —Ç–æ–∫–µ–Ω–æ–≤)")
        else:
            failed_count += 1
            print(f"    ‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")

        time.sleep(random.uniform(0.5, 2.5))

    # –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê
    print(f"\n{'='*70}")
    print(f"üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ë–û–†–ê")
    print(f"{'='*70}")
    print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(corpus_data)}")
    print(f"  ‚ùå –û—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞: {failed_count}")
    print(f"  ‚ö†Ô∏è  –û—Ç–∫–ª–æ–Ω–µ–Ω–æ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞): {too_short_count}")
    print(f"  üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {i+1}/{len(unique_urls)}")
    
    if len(corpus_data) < target_docs:
        print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –°–æ–±—Ä–∞–Ω–æ {len(corpus_data)} –∏–∑ {target_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —É–≤–µ–ª–∏—á—å—Ç–µ target_urls –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏")

    # –°–û–•–†–ê–ù–ï–ù–ò–ï –ö–û–†–ü–£–°–ê –í TXT
    print(f"\nüíæ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞: game_corpus_{len(corpus_data)}.txt")
    with open(f'game_corpus_{len(corpus_data)}.txt', 'w', encoding='utf-8') as f:
        for doc in corpus_data:
            f.write(f"{'='*80}\n")
            f.write(f"–î–û–ö–£–ú–ï–ù–¢ {doc['doc_id']} | {doc['game']}\n")
            f.write(f"{'='*80}\n")
            f.write(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {doc['title']}\n")
            f.write(f"–ê–≤—Ç–æ—Ä: {doc['author']}\n")
            f.write(f"–î–∞—Ç–∞: {doc['date']}\n")
            f.write(f"–¢–æ–∫–µ–Ω–æ–≤: {doc['tokens_count']}\n")
            f.write(f"URL: {doc['url']}\n")
            f.write(f"{'-'*80}\n")
            f.write(f"–ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢ (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤):\n")
            f.write(f"{doc['raw_text']}\n")
            f.write(f"{'-'*80}\n")
            f.write(f"–û–ß–ò–©–ï–ù–ù–´–ô –¢–ï–ö–°–¢:\n")
            f.write(f"{doc['cleaned_text']}\n")
            f.write(f"{'='*80}\n\n")
    print(f"  ‚úÖ TXT –∫–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(corpus_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –°–û–•–†–ê–ù–ï–ù–ò–ï –í CSV
    df = pd.DataFrame(corpus_data)
    df.to_csv(f'game_corpus_{len(corpus_data)}.csv', index=False, encoding='utf-8-sig')
    print(f"  ‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: game_corpus_{len(corpus_data)}.csv")

    # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    total_tokens = sum(doc['tokens_count'] for doc in corpus_data)
    game_counts = df['game'].value_counts()
    author_counts = df['author'].value_counts()

    print(f"\n{'='*70}")
    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–†–ü–£–°–ê")
    print(f"{'='*70}")
    print(f"  üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(corpus_data)}")
    print(f"  üî§ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens:,}")
    print(f"  üìà –°—Ä–µ–¥–Ω. —Ç–æ–∫–µ–Ω–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {total_tokens / len(corpus_data):.2f}" if corpus_data else "  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
    print(f"  üîó –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(parsed_urls)}")
    print(f"  ‚úçÔ∏è  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(author_counts)}")
    
    print(f"\nüéÆ –¢–æ–ø-10 –∏–≥—Ä –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    for game, count in game_counts.head(10).items():
        print(f"    {game}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    print(f"\n‚úçÔ∏è  –¢–æ–ø-10 –∞–≤—Ç–æ—Ä–æ–≤:")
    for author, count in author_counts.head(10).items():
        print(f"    {author}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    print(f"\nüéâ –ö–æ—Ä–ø—É—Å –≥–æ—Ç–æ–≤ –¥–ª—è NLP-–∞–Ω–∞–ª–∏–∑–∞ (LDA, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ)!")
    print(f"üìÅ –§–∞–π–ª—ã: game_corpus_{len(corpus_data)}.txt, game_corpus_{len(corpus_data)}.csv")

if __name__ == "__main__":
    main()