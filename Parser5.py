import warnings
warnings.filterwarnings("ignore")

# LIBRARIES
import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import time
import random
import re
import string
import pandas as pd
import os
from urllib.parse import urljoin, urlparse, quote_plus
from datetime import datetime
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
import pymorphy2
import spacy
import inspect

print("üîß Initializing libraries...")

# PATCH FOR PYMORPHY2
def patch_pymorphy2():
    def getargspec_patch(func):
        try:
            args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)
            return args, varargs, varkw, defaults
        except Exception:
            return [], None, None, None
    inspect.getargspec = getargspec_patch

patch_pymorphy2()

# NLTK SETUP
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("Downloading NLTK resources...")
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    nltk.download('stopwords', quiet=True)
print("  ‚úÖ NLTK ready")

# PYMORPHY2 SETUP
morph = pymorphy2.MorphAnalyzer()
print("  ‚úÖ Pymorphy2 ready")

# SPACY SETUP
try:
    nlp = spacy.load('en_core_web_sm')
except:
    print("Downloading spaCy model: python -m spacy download en_core_web_sm")
    os.system('python -m spacy download en_core_web_sm')
    nlp = spacy.load('en_core_web_sm')
print("  ‚úÖ spaCy ready")

# STOP WORDS
stop_words = set(stopwords.words('english') + stopwords.words('russian'))
print("  ‚úÖ Stop words loaded")

# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ò–ì–† (50+ –∏–≥—Ä –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞)
GAMES = [
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ
    "Hollow Knight", "Hollow Knight Silksong", "Platypus",
    "Hard Truck Apocalypse", "No Man's Sky", "Moonlighter", "Minecraft",
    # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏–Ω–¥–∏
    "Celeste", "Hades", "Stardew Valley", "Undertale", "Terraria",
    "Dead Cells", "Ori and the Blind Forest", "Cuphead", "Shovel Knight",
    "Subnautica", "The Binding of Isaac", "Risk of Rain 2", "Slay the Spire",
    # AAA –∏–≥—Ä—ã
    "The Witcher 3", "Elden Ring", "Dark Souls", "Sekiro", "Bloodborne",
    "God of War", "Red Dead Redemption 2", "Cyberpunk 2077", "Skyrim",
    "Grand Theft Auto V", "The Last of Us", "Horizon Zero Dawn",
    # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏
    "Civilization VI", "StarCraft II", "Age of Empires IV", "Total War",
    "XCOM 2", "Crusader Kings III", "Cities Skylines",
    # –®—É—Ç–µ—Ä—ã
    "Counter-Strike 2", "Valorant", "Apex Legends", "Overwatch 2",
    "Call of Duty", "Battlefield", "Destiny 2", "Halo Infinite",
    # MMORPG/–û–Ω–ª–∞–π–Ω
    "World of Warcraft", "Final Fantasy XIV", "Guild Wars 2", "Dota 2", "League of Legends"
]

# –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê –°–°–´–õ–û–ö –ß–ï–†–ï–ó DUCKDUCKGO (–±–µ–∑ API –∫–ª—é—á–µ–π!)
def search_duckduckgo(query, num_results=15):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ DuckDuckGo"""
    try:
        search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        links = []
        for result in soup.find_all('a', class_='result__a', limit=num_results):
            href = result.get('href')
            if href and href.startswith('http'):
                links.append(href)

        print(f"    üîç DuckDuckGo: –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –¥–ª—è '{query}'")
        return links
    except Exception as e:
        print(f"    ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}")
        return []

# –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê –ß–ï–†–ï–ó BING (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
def search_bing(query, num_results=15):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ Bing"""
    try:
        search_url = f"https://www.bing.com/search?q={quote_plus(query)}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        links = []
        for result in soup.find_all('li', class_='b_algo', limit=num_results):
            link = result.find('a')
            if link and link.get('href'):
                href = link.get('href')
                if href.startswith('http'):
                    links.append(href)

        print(f"    üîç Bing: –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –¥–ª—è '{query}'")
        return links
    except Exception as e:
        print(f"    ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Bing: {e}")
        return []

# –°–ë–û–† –ë–ê–ó–û–í–´–• URL (—Å—Ç–∞—Ç–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
def get_base_urls():
    """–ë–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    base_urls = []

    # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∏–≥—Ä—ã
    url_templates = [
        "https://en.wikipedia.org/wiki/{game}",
        "https://store.steampowered.com/search/?term={game}",
        "https://www.metacritic.com/search/{game}",
        "https://www.ign.com/games/{game}",
        "https://www.pcgamer.com/search/?searchTerm={game}",
        "https://www.gamespot.com/search/?q={game}",
        "https://www.gamesradar.com/search/?q={game}",
        "https://www.rockpapershotgun.com/?s={game}",
    ]

    for game in GAMES:
        game_slug = game.lower().replace(' ', '-').replace("'", '')
        for template in url_templates:
            url = template.format(game=game_slug)
            base_urls.append(url)

    return base_urls

# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –°–ë–û–†–ê URL
def collect_urls(target_count=1500):
    """–°–æ–±–∏—Ä–∞–µ—Ç URLs –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è target_count"""
    print(f"\nüì° Collecting URLs (target: {target_count})...")
    all_urls = set()

    # 1. –ë–∞–∑–æ–≤—ã–µ URL
    print("\n1Ô∏è‚É£ Adding base URLs...")
    base_urls = get_base_urls()
    all_urls.update(base_urls)
    print(f"  ‚úÖ Base URLs: {len(all_urls)}")

    # 2. –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo –∏ Bing
    print("\n2Ô∏è‚É£ Searching via DuckDuckGo & Bing...")
    search_queries = []

    # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∏–≥—Ä
    for game in GAMES[:30]:  # –ü–µ—Ä–≤—ã–µ 30 –∏–≥—Ä
        search_queries.extend([
            f"{game} game review",
            f"{game} gameplay",
            f"{game} wiki",
        ])

    # –û–±—â–∏–µ –∏–≥—Ä–æ–≤—ã–µ —Ç–µ–º—ã
    general_topics = [
        "indie game reviews", "best video games 2024", "gaming news",
        "game development", "esports", "gaming culture",
        "retro gaming", "game design", "video game history",
        "gaming industry", "game mechanics", "RPG games",
        "action games", "strategy games", "simulation games"
    ]
    search_queries.extend(general_topics)

    # –ü–æ–∏—Å–∫
    for query in search_queries[:50]:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ 50 –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(all_urls) >= target_count:
            break

        # DuckDuckGo
        ddg_links = search_duckduckgo(query, num_results=15)
        all_urls.update(ddg_links)
        time.sleep(random.uniform(2, 4))  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        # Bing (–¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è)
        if len(all_urls) < target_count and random.random() > 0.5:
            bing_links = search_bing(query, num_results=15)
            all_urls.update(bing_links)
            time.sleep(random.uniform(2, 4))

        print(f"  üìä Total URLs collected: {len(all_urls)}")

    # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
    filtered_urls = []
    blocked_domains = ['youtube.com', 'twitter.com', 'facebook.com', 'instagram.com',
                      'reddit.com', 'discord.com', 'tiktok.com', 'pinterest.com']

    for url in all_urls:
        domain = urlparse(url).netloc
        if not any(blocked in domain for blocked in blocked_domains):
            filtered_urls.append(url)

    print(f"\n‚úÖ Final URL count: {len(filtered_urls)} (filtered from {len(all_urls)})")
    return filtered_urls[:target_count]

# HTTP SESSION
def create_http_session():
    session = requests.Session()
    ua = UserAgent()
    session.headers.update({'User-Agent': ua.random})
    return session

# PARSE PAGE
def parse_page(url, session, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()

            title = soup.title.string.strip() if soup.title else 'No Title'
            content = soup.find_all(['p', 'article', 'div'], limit=100)
            raw_text = ' '.join([elem.get_text(strip=True) for elem in content if elem.get_text(strip=True)])

            if not raw_text or len(raw_text) < 150:
                return None

            date = datetime.now().strftime('%Y-%m-%d')
            return {'title': title, 'raw_text': raw_text[:8000], 'date': date}
        except (requests.RequestException, Exception) as e:
            if attempt < max_retries - 1:
                time.sleep(random.uniform(1, 3))
    return None

# CLEAN AND LEMMATIZE TEXT
def clean_text(raw_text, is_russian=False):
    text = re.sub(r'<[^>]+>', ' ', raw_text)
    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
    text = re.sub(r'\d+\.?\d*', ' ', text)
    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)
    text = re.sub(r'\s+', ' ', text.strip())

    tokens = word_tokenize(text.lower())
    if is_russian:
        tokens = [morph.parse(token)[0].normal_form for token in tokens
                 if token.isalpha() and token not in stop_words]
    else:
        doc = nlp(' '.join(tokens[:500]))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è spaCy
        tokens = [token.lemma_ for token in doc
                 if token.is_alpha and token.text not in stop_words]

    return ' '.join(tokens[:300]), len(tokens[:300])

# DETERMINE GAME
def get_game_from_url(url, title):
    text_to_check = (url + ' ' + title).lower()
    for game in GAMES:
        if game.lower().replace(' ', '') in text_to_check.replace(' ', ''):
            return game
    return "Gaming General"

# MAIN PROCESS
def main():
    print("\n" + "="*60)
    print("üéÆ GAME CORPUS SCRAPER - 1000+ DOCUMENTS")
    print("="*60)

    # –°–±–æ—Ä URL
    target_docs = 1000
    target_urls = 1500  # –°–æ–±–∏—Ä–∞–µ–º –±–æ–ª—å—à–µ, —Ç.–∫. –Ω–µ –≤—Å–µ —Ä–∞—Å–ø–∞—Ä—Å—è—Ç—Å—è
    unique_urls = collect_urls(target_count=target_urls)

    print(f"\nüöÄ Starting scraping {len(unique_urls)} URLs (target: {target_docs} docs)...")
    session = create_http_session()
    corpus_data = []
    parsed_urls = set()
    doc_id = 1

    random.shuffle(unique_urls)

    for i, url in enumerate(unique_urls):
        if doc_id > target_docs:
            break

        if url in parsed_urls:
            continue

        if i % 50 == 0:
            print(f"\nüìä Progress: {i}/{len(unique_urls)} URLs | {doc_id-1} documents collected")

        print(f"  üîç [{doc_id}] {url[:80]}...")
        parsed = parse_page(url, session)

        if parsed and parsed['raw_text']:
            is_russian = 'ru.' in url or bool(re.search('[–∞-—è–ê-–Ø]', parsed['raw_text'][:500]))
            cleaned_text, token_count = clean_text(parsed['raw_text'], is_russian)

            if token_count > 20:  # –ú–∏–Ω–∏–º—É–º 20 —Ç–æ–∫–µ–Ω–æ–≤
                game = get_game_from_url(url, parsed['title'])
                corpus_data.append({
                    'doc_id': doc_id,
                    'game': game,
                    'title': parsed['title'][:150],
                    'url': url,
                    'raw_text': parsed['raw_text'][:1500],
                    'cleaned_text': cleaned_text,
                    'tokens_count': token_count,
                    'date': parsed['date']
                })
                parsed_urls.add(url)
                print(f"    ‚úÖ Doc {doc_id}: {token_count} tokens ({game})")
                doc_id += 1
            else:
                print(f"    ‚ö† Skipped: insufficient text ({token_count} tokens)")
        else:
            print(f"    ‚ö† Skipped: scraping failed")

        time.sleep(random.uniform(0.5, 2))  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    # SAVE CORPUS TO TXT
    print(f"\nüíæ Creating corpus: game_corpus_{len(corpus_data)}.txt")
    with open(f'game_corpus_{len(corpus_data)}.txt', 'w', encoding='utf-8') as f:
        for doc in corpus_data:
            f.write(f"=== Document {doc['doc_id']} | {doc['game']} | {doc['title']} ===\n")
            f.write(f"URL: {doc['url']}\n")
            f.write(f"Tokens: {doc['tokens_count']} | Date: {doc['date']}\n")
            f.write(f"{doc['cleaned_text']}\n")
            f.write("="*80 + "\n\n")
    print(f"  ‚úÖ TXT corpus saved: {len(corpus_data)} documents")

    # SAVE TO CSV
    df = pd.DataFrame(corpus_data)
    df.to_csv(f'game_corpus_{len(corpus_data)}.csv', index=False, encoding='utf-8')
    print(f"  ‚úÖ CSV saved: game_corpus_{len(corpus_data)}.csv")

    # STATISTICS
    total_tokens = sum(doc['tokens_count'] for doc in corpus_data)
    game_counts = df['game'].value_counts()

    print(f"\n" + "="*60)
    print(f"üìä CORPUS STATISTICS")
    print("="*60)
    print(f"  üìÑ Documents: {len(corpus_data)}")
    print(f"  üî§ Total tokens: {total_tokens:,}")
    print(f"  üìà Avg tokens/doc: {total_tokens / len(corpus_data):.2f}" if corpus_data else "  No data")
    print(f"  üîó Unique URLs: {len(parsed_urls)}")
    print(f"\nüéÆ Top 10 games by document count:")
    for game, count in game_counts.head(10).items():
        print(f"    {game}: {count} docs")
    print(f"\nüéâ Corpus ready for NLP (LDA, clustering, topic modeling)!")

if __name__ == "__main__":
    main()