{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# –ë–ò–ë–õ–ò–û–¢–ï–ö–ò\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import inspect\n",
    "\n",
    "print(\"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫...\")\n",
    "\n",
    "# –§–ò–ö–° NLTK\n",
    "try:\n",
    " nltk.data.find('tokenizers/punkt')\n",
    " nltk.data.find('tokenizers/punkt_tab')\n",
    " nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    " print(\"–°–∫–∞—á–∏–≤–∞–µ–º NLTK —Ä–µ—Å—É—Ä—Å—ã...\")\n",
    " nltk.download('punkt', quiet=True)\n",
    " nltk.download('punkt_tab', quiet=True)\n",
    " nltk.download('stopwords', quiet=True)\n",
    "print(\" ‚úÖ NLTK –≥–æ—Ç–æ–≤\")\n",
    "\n",
    "# FIK PYMORPHY2 (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ AttributeError)\n",
    "def patch_pymorphy2():\n",
    " def getargspec_patch(func):\n",
    "    try:\n",
    " args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)\n",
    " return args, varargs, varkw, defaults\n",
    " except Exception:\n",
    " return [], None, None, None\n",
    " inspect.getargspec = getargspec_patch\n",
    "\n",
    "patch_pymorphy2()\n",
    "\n",
    "# PYMORPHY2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(\" ‚úÖ Pymorphy2 –≥–æ—Ç–æ–≤\")\n",
    "\n",
    "# SPACY\n",
    "try:\n",
    " nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    " print(\"–°–∫–∞—á–∏–≤–∞–µ–º spaCy –º–æ–¥–µ–ª—å: python -m spacy download en_core_web_sm\")\n",
    " os.system('python -m spacy download en_core_web_sm')\n",
    " nlp = spacy.load('en_core_web_sm')\n",
    "print(\" ‚úÖ spaCy –≥–æ—Ç–æ–≤\")\n",
    "\n",
    "# –°–¢–û–ü-–°–õ–û–í–ê\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('russian'))\n",
    "print(\" ‚úÖ –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "\n",
    "# –°–ü–ò–°–û–ö –ò–ì–†\n",
    "GAMES = [\n",
    " \"Hollow Knight\", \"Hollow Knight Silksong\", \"Platypus\",\n",
    " \"Hard Truck Apocalypse\", \"No Man's Sky\", \"Moonlighter\", \"Minecraft\"\n",
    "]\n",
    "\n",
    "# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö URL (50+ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö, *30 = 1500 –ø–æ–ø—ã—Ç–æ–∫)\n",
    "BASE_URLS = [\n",
    " # Hollow Knight (10 URL)\n",
    " \"https://en.wikipedia.org/wiki/Hollow_Knight\",\n",
    " \"https://hollowknight.fandom.com/wiki/Hollow_Knight\",\n",
    " \"https://store.steampowered.com/app/367520/Hollow_Knight\",\n",
    " \"https://www.metacritic.com/game/pc/hollow-knight\",\n",
    " \"https://www.ign.com/articles/2018/06/22/hollow-knight-review\",\n",
    " \"https://www.pcgamer.com/hollow-knight-review\",\n",
    " \"https://www.gamespot.com/reviews/hollow-knight-review-an-exceptional-adventure/1900-6416972\",\n",
    " \"https://www.eurogamer.net/hollow-knight-review\",\n",
    " \"https://www.polygon.com/2017/6/23/15856280/hollow-knight-review-pc\",\n",
    " \"https://www.reddit.com/r/HollowKnight/comments/8s7j4k/hollow_knight_review_thread/\",\n",
    " \n",
    " # Hollow Knight Silksong (10 URL)\n",
    " \"https://hollowknight.fandom.com/wiki/Silksong\",\n",
    " \"https://www.pcgamer.com/games/action/hollow-knight-silksong-review\",\n",
    " \"https://www.metacritic.com/game/pc/hollow-knight-silksong\",\n",
    " \"https://www.ign.com/articles/hollow-knight-silksong-review\",\n",
    " \"https://www.gamespot.com/reviews/hollow-knight-silksong-review/1900-6416980\",\n",
    " \"https://www.eurogamer.net/hollow-knight-silksong-review\",\n",
    " \"https://www.polygon.com/silksong-review-hollow-knight-polygon-score-metacritic\",\n",
    " \"https://store.steampowered.com/app/2028990/Hollow_Knight_Silksong\",\n",
    " \"https://www.reddit.com/r/Silksong/comments/1f5j8k/hollow_knight_silksong_review_thread/\",\n",
    " \"https://www.nintendolife.com/reviews/nintendo-switch/hollow-knight-silksong\",\n",
    " \n",
    " # Platypus (5 URL)\n",
    " \"https://en.wikipedia.org/wiki/Platypus_(video_game)\",\n",
    " \"https://store.steampowered.com/app/307340/Platypus\",\n",
    " \"https://www.mobygames.com/game/10766/platypus\",\n",
    " \"https://www.metacritic.com/game/pc/platypus\",\n",
    " \"https://www.ign.com/games/platypus\",\n",
    " \n",
    " # Hard Truck Apocalypse (5 URL)\n",
    " \"https://en.wikipedia.org/wiki/Hard_Truck_Apocalypse\",\n",
    " \"https://store.steampowered.com/app/307320/Hard_Truck_Apocalypse\",\n",
    " \"https://www.mobygames.com/game/14994/hard-truck-apocalypse\",\n",
    " \"https://www.metacritic.com/game/pc/hard-truck-apocalypse\",\n",
    " \"https://www.ign.com/games/hard-truck-apocalypse\",\n",
    " \n",
    " # No Man's Sky (10 URL)\n",
    " \"https://en.wikipedia.org/wiki/No_Man%27s_Sky\",\n",
    " \"https://www.nomanssky.com/news\",\n",
    " \"https://store.steampowered.com/app/275850/No_Mans_Sky\",\n",
    " \"https://www.ign.com/articles/no-mans-sky-review\",\n",
    " \"https://www.pcgamer.com/no-mans-sky-review\",\n",
    " \"https://www.gamespot.com/reviews/no-mans-sky-review/1900-6416492\",\n",
    " \"https://www.eurogamer.net/no-mans-sky-review\",\n",
    " \"https://www.polygon.com/2016/8/12/12461520/no-mans-sky-review-ps4-playstation-4-pc-windows-hello-games-sony\",\n",
    " \"https://www.metacritic.com/game/pc/no-mans-sky\",\n",
    " \"https://www.reddit.com/r/NoMansSkyTheGame/comments/1f5j8k/no_mans_sky_review_thread/\",\n",
    " \n",
    " # Moonlighter (5 URL)\n",
    " \"https://en.wikipedia.org/wiki/Moonlighter_(video_game)\",\n",
    " \"https://store.steampowered.com/app/606150/Moonlighter\",\n",
    " \"https://www.ign.com/games/moonlighter\",\n",
    " \"https://www.pcgamer.com/moonlighter-review\",\n",
    " \"https://www.gamespot.com/reviews/moonlighter-review-open-for-business/1900-6416930\",\n",
    " \n",
    " # Minecraft (10 URL)\n",
    " \"https://en.wikipedia.org/wiki/Minecraft\",\n",
    " \"https://www.minecraft.net/en-us\",\n",
    " \"https://minecraft.fandom.com/wiki/Minecraft_Wiki\",\n",
    " \"https://www.ign.com/games/minecraft\",\n",
    " \"https://www.pcgamer.com/minecraft-review\",\n",
    " \"https://www.gamespot.com/reviews/minecraft-review/1900-6346734\",\n",
    " \"https://www.eurogamer.net/minecraft-review\",\n",
    " \"https://store.steampowered.com/app/221100/Minecraft\",\n",
    " \"https://www.metacritic.com/game/pc/minecraft\",\n",
    " \"https://www.reddit.com/r/Minecraft/comments/1f5j8k/minecraft_review_thread/\",\n",
    "] * 30 # 1500 –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–¥–ª—è 1000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)\n",
    "\n",
    "# –£–ë–ò–†–ê–ï–ú –î–£–ë–õ–ò–ö–ê–¢–´ –ò –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú\n",
    "unique_urls = list(dict.fromkeys(BASE_URLS))[:1500]\n",
    "print(f\"\\nüìö URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(unique_urls)} (—Ü–µ–ª—å: 1000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)\")\n",
    "\n",
    "# HTTP –°–ï–°–°–ò–Ø\n",
    "def create_http_session():\n",
    " session = requests.Session()\n",
    " ua = UserAgent()\n",
    " session.headers.update({'User-Agent': ua.random})\n",
    " return session\n",
    "\n",
    "# –ü–ê–†–°–ò–ù–ì –°–¢–†–ê–ù–ò–¶–´\n",
    "def parse_page(url, session, max_retries=3):\n",
    " for attempt in range(max_retries):\n",
    "    try:\n",
    " response = session.get(url, timeout=10)\n",
    " response.raise_for_status()\n",
    " soup = BeautifulSoup(response.content, 'html.parser')\n",
    " title = soup.title.string.strip() if soup.title else 'No Title'\n",
    " content = soup.find_all(['p', 'div', 'article'])\n",
    " raw_text = ' '.join([elem.get_text(strip=True) for elem in content if elem.get_text(strip=True)])\n",
    " if not raw_text or len(raw_text) < 100:\n",
    "    return None\n",
    " date = datetime.now().strftime('%Y-%m-%d')\n",
    " return {'title': title, 'raw_text': raw_text[:5000], 'date': date}\n",
    " except (requests.RequestException, Exception) as e:\n",
    " print(f\"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}\")\n",
    " if attempt < max_retries - 1:\n",
    " time.sleep(random.uniform(1, 3))\n",
    " return None\n",
    "\n",
    "# –û–ß–ò–°–¢–ö–ê –ò –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø\n",
    "def clean_text(raw_text, is_russian=False):\n",
    " text = re.sub(r'<[^>]+>', ' ', raw_text)\n",
    " text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)\n",
    " text = re.sub(r'\\d+\\.?\\d*', ' ', text)\n",
    " text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    " text = re.sub(r'\\s+', ' ', text.strip())\n",
    " \n",
    " tokens = word_tokenize(text.lower())\n",
    " if is_russian:\n",
    " tokens = [morph.parse(token)[0].normal_form for token in tokens if token.isalpha() and token not in stop_words]\n",
    " else:\n",
    " doc = nlp(' '.join(tokens))\n",
    " tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
    " \n",
    " return ' '.join(tokens[:200]), len(tokens[:200])\n",
    "\n",
    "# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ò–ì–†–´\n",
    "def get_game_from_url(url, title):\n",
    " for game in GAMES:\n",
    " if game.lower().replace(' ', '') in url.lower().replace(' ', '') or game.lower().replace(' ', '') in title.lower().replace(' ', ''):\n",
    " return game\n",
    " return \"Unknown\"\n",
    "\n",
    "# –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–°\n",
    "def main():\n",
    " print(f\"\\nüöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ 1500+ URL (—Ü–µ–ª—å: 1000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...\")\n",
    " session = create_http_session()\n",
    " corpus_data = []\n",
    " parsed_urls = set()\n",
    " doc_id = 1\n",
    "\n",
    " random.shuffle(unique_urls)\n",
    " for url in unique_urls:\n",
    " if doc_id > 1000:\n",
    " break\n",
    " if url in parsed_urls:\n",
    " continue\n",
    " print(f\" üîç –ü–∞—Ä—Å–∏–Ω–≥ {url}...\")\n",
    " parsed = parse_page(url, session)\n",
    " if parsed and parsed['raw_text']:\n",
    " is_russian = 'ru.' in url or 'russian' in parsed['raw_text'].lower()\n",
    " cleaned_text, token_count = clean_text(parsed['raw_text'], is_russian)\n",
    " if token_count > 10:\n",
    " game = get_game_from_url(url, parsed['title'])\n",
    " corpus_data.append({\n",
    " 'doc_id': doc_id,\n",
    " 'game': game,\n",
    " 'title': parsed['title'][:100],\n",
    " 'url': url,\n",
    " 'raw_text': parsed['raw_text'][:100 0],\n",
    " 'cleaned_text': cleaned_text,\n",
    " 'tokens_count': token_count,\n",
    " 'date': parsed['date']\n",
    " })\n",
    " parsed_urls.add(url)\n",
    " print(f\" ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –¥–æ–±–∞–≤–ª–µ–Ω: {token_count} —Ç–æ–∫–µ–Ω–æ–≤ ({game})\")\n",
    " doc_id += 1\n",
    " else:\n",
    " print(f\" ‚ö† –ü—Ä–æ–ø—É—â–µ–Ω: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞\")\n",
    " else:\n",
    " print(f\" ‚ö† –ü—Ä–æ–ø—É—â–µ–Ω: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞\")\n",
    " time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    " # –°–û–•–†–ê–ù–ï–ù–ò–ï –ö–û–†–ü–£–°–ê –í TXT\n",
    " print(f\"\\nüíæ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞: game_corpus_1000.txt\")\n",
    " with open('game_corpus_1000.txt', 'w', encoding='utf-8') as f:\n",
    " for doc in corpus_data:\n",
    " f.write(f\"=== Document {doc['doc_id']} | {doc['game']} | {doc['title']} | {doc['url']} ===\\n\")\n",
    " f.write(f\"Tokens: {doc['tokens_count']} | Date: {doc['date']}\\n\")\n",
    " f.write(f\"{doc['cleaned_text']}\\n---\\n\")\n",
    " print(f\" ‚úÖ TXT-–∫–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(corpus_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "\n",
    " # –°–û–•–†–ê–ù–ï–ù–ò–ï –í CSV\n",
    " df = pd.DataFrame(corpus_data)\n",
    " df.to_csv('game_corpus_1000.csv', index=False, encoding='utf-8')\n",
    " print(f\" ‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: game_corpus_1000.csv\")\n",
    "\n",
    " # –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n",
    " total_tokens = sum(doc['tokens_count'] for doc in corpus_data)\n",
    " print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞:\")\n",
    " print(f\" –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(corpus_data)}\")\n",
    " print(f\" –¢–æ–∫–µ–Ω–æ–≤: {total_tokens}\")\n",
    " print(f\" –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {total_tokens / len(corpus_data):.2f}\" if corpus_data else \" –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö\")\n",
    " print(f\" –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(parsed_urls)}\")\n",
    " print(f\"\\nüéâ –ö–æ—Ä–ø—É—Å –≥–æ—Ç–æ–≤ –¥–ª—è NLP (LDA, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
