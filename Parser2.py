import warnings
warnings.filterwarnings("ignore")

# –ë–ò–ë–õ–ò–û–¢–ï–ö–ò
import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import time
import random
import re
import string
import pandas as pd
import os
from urllib.parse import urljoin
from datetime import datetime
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
import pymorphy2
import spacy
import inspect
from googleapiclient.discovery import build  # –î–ª—è Google Search API

print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫...")

# –§–ò–ö–° NLTK
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("–°–∫–∞—á–∏–≤–∞–µ–º NLTK —Ä–µ—Å—É—Ä—Å—ã...")
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    nltk.download('stopwords', quiet=True)
print("  ‚úÖ NLTK –≥–æ—Ç–æ–≤")

# –§–ò–ö–° PYMORPHY2
original_getargspec = inspect.getargspec
def getargspec_patch(func):
    if func.__name__ == '__init__':
        full_args = inspect.getfullargspec(func)
        return full_args.args, full_args.varargs, full_args.varkw, full_args.defaults
    else:
        return original_getargspec(func)
inspect.getargspec = getargspec_patch

morph = pymorphy2.MorphAnalyzer()
print("  ‚úÖ Pymorphy2 –≥–æ—Ç–æ–≤")

# SPACY –î–õ–Ø –ê–ù–ì–õ–ò–ô–°–ö–û–ì–û
try:
    nlp = spacy.load('en_core_web_sm')
except:
    print("–°–∫–∞—á–∏–≤–∞–µ–º spaCy –º–æ–¥–µ–ª—å: python -m spacy download en_core_web_sm")
    os.system('python -m spacy download en_core_web_sm')
    nlp = spacy.load('en_core_web_sm')
print("  ‚úÖ spaCy –≥–æ—Ç–æ–≤")

# –°–¢–û–ü-–°–õ–û–í–ê
stop_words = set(stopwords.words('english') + stopwords.words('russian'))
print("  ‚úÖ –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# –°–ü–ò–°–û–ö –ò–ì–†
GAMES = [
    "Hollow Knight", "Hollow Knight Silksong", "Platypus",
    "Hard Truck Apocalypse", "No Man's Sky", "Moonlighter", "Minecraft"
]

# GOOGLE SEARCH API (–¥–ª—è 1000 URL)
def get_search_urls(query, api_key, cse_id, num_results=10):
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        result = service.cse().list(q=query, cx=cse_id, num=num_results).execute()
        urls = [item['link'] for item in result.get('items', [])]
        return urls
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ Google Search API: {e}")
        return []

# –ó–ê–ì–õ–£–®–ö–ê URL (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –±–µ–∑ API)
BASE_URLS = [
    # Hollow Knight
    "https://en.wikipedia.org/wiki/Hollow_Knight",
    "https://hollowknight.fandom.com/wiki/Hollow_Knight",
    "https://store.steampowered.com/app/367520/Hollow_Knight",
    "https://www.metacritic.com/game/pc/hollow-knight",
    "https://www.ign.com/articles/2018/06/22/hollow-knight-review",
    "https://www.pcgamer.com/hollow-knight-review",
    "https://www.gamespot.com/reviews/hollow-knight-review-an-exceptional-adventure/1900-6416972",
    # Hollow Knight Silksong
    "https://hollowknight.fandom.com/wiki/Silksong",
    "https://www.pcgamer.com/games/action/hollow-knight-silksong-review",
    "https://www.metacritic.com/game/pc/hollow-knight-silksong",
    # Platypus
    "https://en.wikipedia.org/wiki/Platypus_(video_game)",
    "https://store.steampowered.com/app/307340/Platypus",
    "https://www.mobygames.com/game/10766/platypus",
    # Hard Truck Apocalypse
    "https://en.wikipedia.org/wiki/Hard_Truck_Apocalypse",
    "https://store.steampowered.com/app/307320/Hard_Truck_Apocalypse",
    "https://www.mobygames.com/game/14994/hard-truck-apocalypse",
    # No Man's Sky
    "https://en.wikipedia.org/wiki/No_Man%27s_Sky",
    "https://www.nomanssky.com/news",
    "https://store.steampowered.com/app/275850/No_Mans_Sky",
    "https://www.ign.com/articles/no-mans-sky-review",
    # Moonlighter
    "https://en.wikipedia.org/wiki/Moonlighter_(video_game)",
    "https://store.steampowered.com/app/606150/Moonlighter",
    "https://www.ign.com/games/moonlighter",
    # Minecraft
    "https://en.wikipedia.org/wiki/Minecraft",
    "https://www.minecraft.net/en-us",
    "https://minecraft.fandom.com/wiki/Minecraft_Wiki",
    "https://www.ign.com/games/minecraft",
] * 40  # ~1000 URL (–¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)

# –ü–û–õ–£–ß–ï–ù–ò–ï 1000 URL –ß–ï–†–ï–ó GOOGLE SEARCH API
# –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏ –∫–ª—é—á–∏:
# API_KEY = "YOUR_GOOGLE_API_KEY"
# CSE_ID = "YOUR_CUSTOM_SEARCH_ENGINE_ID"
# urls = []
# for game in GAMES:
#     urls.extend(get_search_urls(game + " game review", API_KEY, CSE_ID, num_results=30))
# unique_urls = list(dict.fromkeys(urls))[:1000]  # 1000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
unique_urls = list(dict.fromkeys(BASE_URLS))[:1000]  # –ó–∞–≥–ª—É—à–∫–∞
print(f"\nüìö –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(unique_urls)}")

# HTTP –°–ï–°–°–ò–Ø
def create_http_session():
    session = requests.Session()
    ua = UserAgent()
    session.headers.update({'User-Agent': ua.random})
    return session

# –ü–ê–†–°–ò–ù–ì –°–¢–†–ê–ù–ò–¶–´
def parse_page(url, session, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.title.string.strip() if soup.title else 'No Title'
            content = soup.find_all(['p', 'div', 'article'])
            raw_text = ' '.join([elem.get_text(strip=True) for elem in content if elem.get_text(strip=True)])
            if not raw_text or len(raw_text) < 100:
                return None
            date = datetime.now().strftime('%Y-%m-%d')  # –ó–∞–≥–ª—É—à–∫–∞
            return {'title': title, 'raw_text': raw_text[:5000], 'date': date}
        except (requests.RequestException, Exception) as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(1, 3))
    return None

# –û–ß–ò–°–¢–ö–ê –ò –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø
def clean_text(raw_text, is_russian=False):
    # –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤, —á–∏—Å–µ–ª, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'<[^>]+>', ' ', raw_text)
    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
    text = re.sub(r'\d+\.?\d*', ' ', text)
    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)
    text = re.sub(r'\s+', ' ', text.strip())
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = word_tokenize(text.lower())
    
    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    if is_russian:
        tokens = [morph.parse(token)[0].normal_form for token in tokens if token.isalpha() and token not in stop_words]
    else:
        doc = nlp(' '.join(tokens))
        tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]
    
    return ' '.join(tokens[:200]), len(tokens[:200])

# –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–°
def main():
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ {len(unique_urls)} —Å—Ç–∞—Ç–µ–π...")
    session = create_http_session()
    corpus_data = []
    parsed_urls = set()
    doc_id = 1

    for url in unique_urls:
        if doc_id > 1000:
            break
        if url in parsed_urls:
            continue
        print(f"  üîç –ü–∞—Ä—Å–∏–Ω–≥ {url}...")
        parsed = parse_page(url, session)
        if parsed and parsed['raw_text']:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–≥—Ä—É
            game = next((g for g in GAMES if g.lower() in url.lower() or g.lower() in parsed['title'].lower()), "Unknown")
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ (—Ä—É—Å—Å–∫–∏–π, –µ—Å–ª–∏ URL —Å–æ–¥–µ—Ä–∂–∏—Ç "ru.")
            is_russian = 'ru.' in url or 'russian' in parsed['raw_text'].lower()
            cleaned_text, token_count = clean_text(parsed['raw_text'], is_russian)
            if token_count > 10:
                corpus_data.append({
                    'doc_id': doc_id,
                    'game': game,
                    'title': parsed['title'][:100],
                    'url': url,
                    'raw_text': parsed['raw_text'][:1000],
                    'cleaned_text': cleaned_text,
                    'tokens_count': token_count,
                    'date': parsed['date']
                })
                parsed_urls.add(url)
                print(f"    ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –¥–æ–±–∞–≤–ª–µ–Ω: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
                doc_id += 1
        time.sleep(random.uniform(0.5, 1.5))  # –ê–Ω—Ç–∏-–±–∞–Ω

    # –°–û–•–†–ê–ù–ï–ù–ò–ï –ö–û–†–ü–£–°–ê –í TXT
    print(f"\nüíæ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞: game_corpus_1000.txt")
    with open('game_corpus_1000.txt', 'w', encoding='utf-8') as f:
        for doc in corpus_data:
            f.write(f"=== Document {doc['doc_id']} | {doc['game']} | {doc['title']} | {doc['url']} ===\n")
            f.write(f"Tokens: {doc['tokens_count']} | Date: {doc['date']}\n")
            f.write(f"{doc['cleaned_text']}\n---\n")
    print(f"  ‚úÖ TXT-–∫–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(corpus_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –°–û–•–†–ê–ù–ï–ù–ò–ï –í CSV
    df = pd.DataFrame(corpus_data)
    df.to_csv('game_corpus_1000.csv', index=False, encoding='utf-8')
    print(f"  ‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: game_corpus_1000.csv")

    # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    total_tokens = sum(doc['tokens_count'] for doc in corpus_data)
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞:")
    print(f"  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(corpus_data)}")
    print(f"  –¢–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {total_tokens / len(corpus_data):.2f}" if corpus_data else "  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
    print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(parsed_urls)}")
    print(f"\nüéâ –ö–æ—Ä–ø—É—Å –≥–æ—Ç–æ–≤ –¥–ª—è NLP (LDA, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)!")

if __name__ == "__main__":
    main()